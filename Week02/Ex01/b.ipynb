{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LAB 02 - B√ÄI 1: Ph√¢n lo·∫°i Thu nh·∫≠p Adult Income Dataset\n",
        "\n",
        "## üìã M·ª•c ti√™u:\n",
        "1. Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu (missing values, encoding, scaling)\n",
        "2. X√¢y d·ª±ng 3 m√¥ h√¨nh ML\n",
        "3. So s√°nh hi·ªáu nƒÉng v·ªõi/kh√¥ng c√≥ ti·ªÅn x·ª≠ l√Ω\n",
        "4. C√¢n b·∫±ng d·ªØ li·ªáu v·ªõi SMOTE\n",
        "5. L∆∞u model t·ªët nh·∫•t\n",
        "6. S·ª≠ d·ª•ng WandB ƒë·ªÉ theo d√µi th√≠ nghi·ªám\n",
        "7. T·∫°o giao di·ªán Gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. C√†i ƒë·∫∑t th∆∞ vi·ªán"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q ucimlrepo wandb scikit-learn pandas numpy matplotlib seaborn imbalanced-learn gradio plotly joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import th∆∞ vi·ªán"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import gradio as gr\n",
        "import joblib\n",
        "\n",
        "# UCI ML Repository\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
        ")\n",
        "\n",
        "# Imbalanced-learn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# WandB\n",
        "import wandb\n",
        "\n",
        "print(\"‚úÖ Import th√†nh c√¥ng!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Kh·ªüi t·∫°o WandB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Login to WandB\n",
        "wandb.login()\n",
        "\n",
        "# Initialize WandB\n",
        "wandb.init(\n",
        "    project=\"adult-income-classification\",\n",
        "    name=\"preprocessing-comparison-balanced\",\n",
        "    config={\n",
        "        \"dataset\": \"Adult Income UCI\",\n",
        "        \"task\": \"Binary Classification\",\n",
        "        \"models\": [\"LogisticRegression\", \"RandomForest\", \"GradientBoosting\"],\n",
        "        \"test_size\": 0.2,\n",
        "        \"random_state\": 42,\n",
        "        \"balancing\": \"SMOTE\"\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"‚úÖ WandB initialized!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. T·∫£i v√† kh√°m ph√° d·ªØ li·ªáu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fetch dataset\n",
        "print(\"üì• ƒêang t·∫£i d·ªØ li·ªáu Adult Income...\")\n",
        "adult = fetch_ucirepo(id=2)\n",
        "\n",
        "# Get features and target\n",
        "X = adult.data.features\n",
        "y = adult.data.targets\n",
        "\n",
        "# ‚ö†Ô∏è FIX: Clean target labels - remove trailing dots and whitespace\n",
        "y['income'] = y['income'].astype(str).str.strip().str.rstrip('.')\n",
        "\n",
        "# Combine into one dataframe\n",
        "data = pd.concat([X, y], axis=1)\n",
        "\n",
        "print(f\"\\nüìä K√≠ch th∆∞·ªõc d·ªØ li·ªáu: {data.shape}\")\n",
        "print(f\"   - S·ªë m·∫´u: {data.shape[0]:,}\")\n",
        "print(f\"   - S·ªë features: {data.shape[1]-1}\")\n",
        "\n",
        "# Display metadata\n",
        "print(\"\\nüìã Metadata:\")\n",
        "print(adult.metadata)\n",
        "\n",
        "print(\"\\nüìä Variable Information:\")\n",
        "print(adult.variables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display first rows\n",
        "print(\"\\nüîç 5 m·∫´u ƒë·∫ßu ti√™n:\")\n",
        "display(data.head())\n",
        "\n",
        "# Data info\n",
        "print(\"\\nüìä Th√¥ng tin d·ªØ li·ªáu:\")\n",
        "data.info()\n",
        "\n",
        "# Statistical summary\n",
        "print(\"\\nüìà Th·ªëng k√™ m√¥ t·∫£:\")\n",
        "display(data.describe())\n",
        "\n",
        "# Check target distribution\n",
        "print(\"\\nüéØ Ph√¢n ph·ªëi nh√£n (SAU KHI CLEAN):\")\n",
        "print(data['income'].value_counts())\n",
        "print(f\"\\n   T·ª∑ l·ªá: {data['income'].value_counts(normalize=True)}\")\n",
        "print(f\"\\n   ‚úÖ Ch·ªâ c√≥ 2 class: {data['income'].unique()}\")\n",
        "print(f\"   ‚úÖ S·ªë l∆∞·ª£ng unique values: {data['income'].nunique()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. EDA - Kh√°m ph√° d·ªØ li·ªáu chi ti·∫øt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Missing values analysis\n",
        "print(\"üîç Ph√¢n t√≠ch gi√° tr·ªã thi·∫øu:\\n\")\n",
        "missing = data.isnull().sum()\n",
        "missing_pct = (missing / len(data)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing,\n",
        "    'Percentage': missing_pct\n",
        "}).sort_values('Missing Count', ascending=False)\n",
        "print(missing_df[missing_df['Missing Count'] > 0])\n",
        "\n",
        "# Visualize missing values\n",
        "fig = px.bar(\n",
        "    missing_df[missing_df['Missing Count'] > 0],\n",
        "    x=missing_df[missing_df['Missing Count'] > 0].index,\n",
        "    y='Percentage',\n",
        "    title='T·ª∑ l·ªá gi√° tr·ªã thi·∫øu theo feature',\n",
        "    labels={'x': 'Features', 'y': 'Percentage (%)'}\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify categorical and numerical columns\n",
        "categorical_cols = data.select_dtypes(include=['object']).columns.tolist()\n",
        "categorical_cols.remove('income')  # Remove target\n",
        "numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "print(f\"\\nüìä Categorical features ({len(categorical_cols)}): {categorical_cols}\")\n",
        "print(f\"\\nüî¢ Numerical features ({len(numerical_cols)}): {numerical_cols}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize target distribution\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(\n",
        "        x=data['income'].value_counts().index,\n",
        "        y=data['income'].value_counts().values,\n",
        "        text=data['income'].value_counts().values,\n",
        "        textposition='auto',\n",
        "    )\n",
        "])\n",
        "fig.update_layout(\n",
        "    title='Ph√¢n ph·ªëi nh√£n Thu nh·∫≠p',\n",
        "    xaxis_title='Income',\n",
        "    yaxis_title='Count'\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "print(f\"‚ö†Ô∏è D·ªØ li·ªáu KH√îNG c√¢n b·∫±ng: {data['income'].value_counts()[0] / data['income'].value_counts()[1]:.2f}:1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution of numerical features\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=3,\n",
        "    subplot_titles=numerical_cols\n",
        ")\n",
        "\n",
        "for i, col in enumerate(numerical_cols, 1):\n",
        "    row = (i-1) // 3 + 1\n",
        "    col_pos = (i-1) % 3 + 1\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Histogram(x=data[col], name=col),\n",
        "        row=row, col=col_pos\n",
        "    )\n",
        "\n",
        "fig.update_layout(height=600, showlegend=False, title_text=\"Ph√¢n ph·ªëi c√°c bi·∫øn s·ªë\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "corr = data[numerical_cols].corr()\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm', center=0)\n",
        "plt.title('Ma tr·∫≠n t∆∞∆°ng quan c√°c bi·∫øn s·ªë')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Chu·∫©n b·ªã d·ªØ li·ªáu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate features and target\n",
        "X_raw = data.drop('income', axis=1).copy()\n",
        "y_raw = data['income'].copy()\n",
        "\n",
        "# Encode target\n",
        "le_target = LabelEncoder()\n",
        "y_encoded = le_target.fit_transform(y_raw)\n",
        "\n",
        "print(\"‚úÖ Encoded target classes:\", le_target.classes_)\n",
        "print(f\"   Mapping: {dict(zip(le_target.classes_, le_target.transform(le_target.classes_)))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1. Tr∆∞·ªùng h·ª£p 1: KH√îNG ti·ªÅn x·ª≠ l√Ω (ch·ªâ encode c∆° b·∫£n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üî¥ TR∆Ø·ªúNG H·ª¢P 1: KH√îNG TI·ªÄN X·ª¨ L√ù\\n\")\n",
        "\n",
        "X_no_prep = X_raw.copy()\n",
        "\n",
        "# Only basic label encoding for categorical features\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    X_no_prep[col] = le.fit_transform(X_no_prep[col].astype(str))\n",
        "\n",
        "print(f\"‚úÖ D·ªØ li·ªáu sau x·ª≠ l√Ω c∆° b·∫£n: {X_no_prep.shape}\")\n",
        "print(f\"   - Missing values c√≤n l·∫°i: {X_no_prep.isnull().sum().sum()}\")\n",
        "print(f\"   - T·∫•t c·∫£ features ƒë√£ ƒë∆∞·ª£c encode\")\n",
        "\n",
        "# Split data\n",
        "X_train_no, X_test_no, y_train_no, y_test_no = train_test_split(\n",
        "    X_no_prep, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä K√≠ch th∆∞·ªõc t·∫≠p d·ªØ li·ªáu:\")\n",
        "print(f\"   - Training: {X_train_no.shape}\")\n",
        "print(f\"   - Testing: {X_test_no.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2. Tr∆∞·ªùng h·ª£p 2: C√ì ti·ªÅn x·ª≠ l√Ω ƒë·∫ßy ƒë·ªß + C√¢n b·∫±ng d·ªØ li·ªáu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üü¢ TR∆Ø·ªúNG H·ª¢P 2: TI·ªÄN X·ª¨ L√ù ƒê·∫¶Y ƒê·ª¶ + C√ÇN B·∫∞NG D·ªÆ LI·ªÜU\\n\")\n",
        "\n",
        "X_prep = X_raw.copy()\n",
        "\n",
        "# Step 1: Handle missing values\n",
        "print(\"1Ô∏è‚É£ X·ª≠ l√Ω gi√° tr·ªã thi·∫øu...\")\n",
        "for col in categorical_cols:\n",
        "    if X_prep[col].isnull().any():\n",
        "        mode_val = X_prep[col].mode()[0]\n",
        "        X_prep[col].fillna(mode_val, inplace=True)\n",
        "\n",
        "for col in numerical_cols:\n",
        "    if X_prep[col].isnull().any():\n",
        "        median_val = X_prep[col].median()\n",
        "        X_prep[col].fillna(median_val, inplace=True)\n",
        "\n",
        "print(f\"   ‚úÖ Kh√¥ng c√≤n missing values: {X_prep.isnull().sum().sum() == 0}\")\n",
        "\n",
        "# Step 2: Handle outliers using IQR\n",
        "print(\"\\n2Ô∏è‚É£ X·ª≠ l√Ω outliers (IQR method)...\")\n",
        "outliers_removed = 0\n",
        "for col in numerical_cols:\n",
        "    Q1 = X_prep[col].quantile(0.25)\n",
        "    Q3 = X_prep[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    \n",
        "    # Cap outliers\n",
        "    before = (X_prep[col] < lower_bound).sum() + (X_prep[col] > upper_bound).sum()\n",
        "    X_prep[col] = X_prep[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "    outliers_removed += before\n",
        "\n",
        "print(f\"   ‚úÖ ƒê√£ x·ª≠ l√Ω {outliers_removed:,} outliers\")\n",
        "\n",
        "# Step 3: One-Hot Encoding\n",
        "print(\"\\n3Ô∏è‚É£ One-Hot Encoding cho categorical features...\")\n",
        "X_prep_encoded = pd.get_dummies(X_prep, columns=categorical_cols, drop_first=True)\n",
        "print(f\"   ‚úÖ Features sau encoding: {X_prep_encoded.shape[1]} (t·ª´ {X_prep.shape[1]})\")\n",
        "\n",
        "# Step 4: Split data first (before SMOTE)\n",
        "X_train_prep, X_test_prep, y_train_prep, y_test_prep = train_test_split(\n",
        "    X_prep_encoded, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "# Step 5: Scale features (fit on train only)\n",
        "print(\"\\n4Ô∏è‚É£ Chu·∫©n h√≥a features (StandardScaler)...\")\n",
        "scaler = StandardScaler()\n",
        "X_train_prep_scaled = scaler.fit_transform(X_train_prep)\n",
        "X_test_prep_scaled = scaler.transform(X_test_prep)\n",
        "print(f\"   ‚úÖ ƒê√£ chu·∫©n h√≥a {X_train_prep.shape[1]} features\")\n",
        "\n",
        "# Step 6: Apply SMOTE on training data only\n",
        "print(\"\\n5Ô∏è‚É£ C√¢n b·∫±ng d·ªØ li·ªáu v·ªõi SMOTE (ch·ªâ tr√™n t·∫≠p train)...\")\n",
        "print(f\"   üìä Ph√¢n ph·ªëi TR∆Ø·ªöC SMOTE: {np.bincount(y_train_prep)}\")\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_prep_scaled, y_train_prep)\n",
        "\n",
        "print(f\"   ‚úÖ Ph√¢n ph·ªëi SAU SMOTE: {np.bincount(y_train_balanced)}\")\n",
        "print(f\"   ‚úÖ ƒê√£ tƒÉng t·ª´ {len(y_train_prep):,} l√™n {len(y_train_balanced):,} samples\")\n",
        "\n",
        "print(f\"\\nüìä K√≠ch th∆∞·ªõc t·∫≠p d·ªØ li·ªáu sau ti·ªÅn x·ª≠ l√Ω:\")\n",
        "print(f\"   - Training (balanced): {X_train_balanced.shape}\")\n",
        "print(f\"   - Testing: {X_test_prep_scaled.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. X√¢y d·ª±ng v√† hu·∫•n luy·ªán m√¥ h√¨nh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "def evaluate_model(name, model, X_train, X_test, y_train, y_test, case=\"\"):\n",
        "    \"\"\"\n",
        "    Train and evaluate a model\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üéØ {name} - {case}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(\"‚è≥ ƒêang hu·∫•n luy·ªán...\")\n",
        "    \n",
        "    # Train\n",
        "    model.fit(X_train, y_train)\n",
        "    print(\"‚úÖ Ho√†n th√†nh hu·∫•n luy·ªán!\")\n",
        "    \n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    \n",
        "    print(f\"\\nüìä K·∫øt qu·∫£ ƒë√°nh gi√°:\")\n",
        "    print(f\"   - Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"   - Precision: {precision:.4f}\")\n",
        "    print(f\"   - Recall:    {recall:.4f}\")\n",
        "    print(f\"   - F1-Score:  {f1:.4f}\")\n",
        "    print(f\"   - ROC AUC:   {roc_auc:.4f}\")\n",
        "    \n",
        "    print(f\"\\nüìã Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=le_target.classes_))\n",
        "    \n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    \n",
        "    # Log to WandB\n",
        "    wandb.log({\n",
        "        f\"{case}_{name}_accuracy\": accuracy,\n",
        "        f\"{case}_{name}_precision\": precision,\n",
        "        f\"{case}_{name}_recall\": recall,\n",
        "        f\"{case}_{name}_f1\": f1,\n",
        "        f\"{case}_{name}_roc_auc\": roc_auc,\n",
        "        f\"{case}_{name}_confusion_matrix\": wandb.plot.confusion_matrix(\n",
        "            probs=None,\n",
        "            y_true=y_test,\n",
        "            preds=y_pred,\n",
        "            class_names=le_target.classes_.tolist()\n",
        "        )\n",
        "    })\n",
        "    \n",
        "    return {\n",
        "        'model': model,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'roc_auc': roc_auc,\n",
        "        'confusion_matrix': cm,\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_proba': y_pred_proba\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1. Hu·∫•n luy·ªán - Tr∆∞·ªùng h·ª£p 1: KH√îNG ti·ªÅn x·ª≠ l√Ω"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üî¥ HU·∫§N LUY·ªÜN M√î H√åNH - TR∆Ø·ªúNG H·ª¢P 1: KH√îNG TI·ªÄN X·ª¨ L√ù\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "results_no_prep = {}\n",
        "for name, model in models.items():\n",
        "    results_no_prep[name] = evaluate_model(\n",
        "        name, model,\n",
        "        X_train_no, X_test_no,\n",
        "        y_train_no, y_test_no,\n",
        "        case=\"No_Preprocessing\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2. Hu·∫•n luy·ªán - Tr∆∞·ªùng h·ª£p 2: C√ì ti·ªÅn x·ª≠ l√Ω + C√¢n b·∫±ng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üü¢ HU·∫§N LUY·ªÜN M√î H√åNH - TR∆Ø·ªúNG H·ª¢P 2: C√ì TI·ªÄN X·ª¨ L√ù + C√ÇN B·∫∞NG\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "results_with_prep = {}\n",
        "for name, model in models.items():\n",
        "    results_with_prep[name] = evaluate_model(\n",
        "        name, model,\n",
        "        X_train_balanced, X_test_prep_scaled,\n",
        "        y_train_balanced, y_test_prep,\n",
        "        case=\"With_Preprocessing\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. So s√°nh k·∫øt qu·∫£"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison dataframe\n",
        "comparison_data = []\n",
        "\n",
        "for name in models.keys():\n",
        "    comparison_data.append({\n",
        "        'Model': name,\n",
        "        'Case': 'Kh√¥ng ti·ªÅn x·ª≠ l√Ω',\n",
        "        'Accuracy': results_no_prep[name]['accuracy'],\n",
        "        'Precision': results_no_prep[name]['precision'],\n",
        "        'Recall': results_no_prep[name]['recall'],\n",
        "        'F1-Score': results_no_prep[name]['f1'],\n",
        "        'ROC AUC': results_no_prep[name]['roc_auc']\n",
        "    })\n",
        "    \n",
        "    comparison_data.append({\n",
        "        'Model': name,\n",
        "        'Case': 'C√≥ ti·ªÅn x·ª≠ l√Ω + C√¢n b·∫±ng',\n",
        "        'Accuracy': results_with_prep[name]['accuracy'],\n",
        "        'Precision': results_with_prep[name]['precision'],\n",
        "        'Recall': results_with_prep[name]['recall'],\n",
        "        'F1-Score': results_with_prep[name]['f1'],\n",
        "        'ROC AUC': results_with_prep[name]['roc_auc']\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "print(\"\\nüìä B·∫¢NG SO S√ÅNH K·∫æT QU·∫¢:\")\n",
        "print(\"=\"*100)\n",
        "display(comparison_df)\n",
        "\n",
        "# Log comparison table to WandB\n",
        "wandb.log({\"comparison_table\": wandb.Table(dataframe=comparison_df)})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize comparison\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC AUC']\n",
        "\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=3,\n",
        "    subplot_titles=metrics,\n",
        "    specs=[[{'type': 'bar'}, {'type': 'bar'}, {'type': 'bar'}],\n",
        "           [{'type': 'bar'}, {'type': 'bar'}, None]]\n",
        ")\n",
        "\n",
        "for i, metric in enumerate(metrics, 1):\n",
        "    row = (i-1) // 3 + 1\n",
        "    col = (i-1) % 3 + 1\n",
        "    \n",
        "    metric_data = comparison_df.pivot(index='Model', columns='Case', values=metric)\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Bar(name='Kh√¥ng ti·ªÅn x·ª≠ l√Ω', x=metric_data.index, y=metric_data['Kh√¥ng ti·ªÅn x·ª≠ l√Ω']),\n",
        "        row=row, col=col\n",
        "    )\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Bar(name='C√≥ ti·ªÅn x·ª≠ l√Ω + C√¢n b·∫±ng', x=metric_data.index, y=metric_data['C√≥ ti·ªÅn x·ª≠ l√Ω + C√¢n b·∫±ng']),\n",
        "        row=row, col=col\n",
        "    )\n",
        "\n",
        "fig.update_layout(height=800, showlegend=True, title_text=\"So s√°nh hi·ªáu nƒÉng c√°c m√¥ h√¨nh\")\n",
        "fig.show()\n",
        "\n",
        "# Log to WandB\n",
        "wandb.log({\"comparison_chart\": fig})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. L∆∞u m√¥ h√¨nh t·ªët nh·∫•t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find best model based on F1-Score\n",
        "best_model_name = max(results_with_prep.items(), key=lambda x: x[1]['f1'])[0]\n",
        "best_model = results_with_prep[best_model_name]['model']\n",
        "best_f1 = results_with_prep[best_model_name]['f1']\n",
        "\n",
        "print(f\"\\nüèÜ M√î H√åNH T·ªêT NH·∫§T: {best_model_name}\")\n",
        "print(f\"   üìä F1-Score: {best_f1:.4f}\")\n",
        "print(f\"   üìä Accuracy: {results_with_prep[best_model_name]['accuracy']:.4f}\")\n",
        "print(f\"   üìä ROC AUC: {results_with_prep[best_model_name]['roc_auc']:.4f}\")\n",
        "\n",
        "# Save model and preprocessing objects\n",
        "print(\"\\nüíæ ƒêang l∆∞u m√¥ h√¨nh v√† preprocessing objects...\")\n",
        "\n",
        "# Save model\n",
        "joblib.dump(best_model, 'best_model.pkl')\n",
        "print(\"   ‚úÖ ƒê√£ l∆∞u: best_model.pkl\")\n",
        "\n",
        "# Save scaler\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "print(\"   ‚úÖ ƒê√£ l∆∞u: scaler.pkl\")\n",
        "\n",
        "# Save label encoder\n",
        "joblib.dump(le_target, 'label_encoder.pkl')\n",
        "print(\"   ‚úÖ ƒê√£ l∆∞u: label_encoder.pkl\")\n",
        "\n",
        "# Save feature names after preprocessing\n",
        "feature_names = X_prep_encoded.columns.tolist()\n",
        "joblib.dump(feature_names, 'feature_names.pkl')\n",
        "print(\"   ‚úÖ ƒê√£ l∆∞u: feature_names.pkl\")\n",
        "\n",
        "# Save to WandB\n",
        "wandb.save('best_model.pkl')\n",
        "wandb.save('scaler.pkl')\n",
        "wandb.save('label_encoder.pkl')\n",
        "wandb.save('feature_names.pkl')\n",
        "\n",
        "print(\"\\n‚úÖ ƒê√£ l∆∞u t·∫•t c·∫£ artifacts l√™n WandB!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. T·∫°o giao di·ªán Gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_income(age, workclass, education, marital_status, occupation,\n",
        "                   relationship, race, sex, capital_gain, capital_loss,\n",
        "                   hours_per_week, native_country):\n",
        "    \"\"\"\n",
        "    Predict income based on input features\n",
        "    \"\"\"\n",
        "    # Create input dataframe\n",
        "    input_data = pd.DataFrame([{\n",
        "        'age': age,\n",
        "        'fnlwgt': 0,  # Not used in prediction\n",
        "        'education-num': 0,  # Will be dropped\n",
        "        'workclass': workclass,\n",
        "        'education': education,\n",
        "        'marital-status': marital_status,\n",
        "        'occupation': occupation,\n",
        "        'relationship': relationship,\n",
        "        'race': race,\n",
        "        'sex': sex,\n",
        "        'capital-gain': capital_gain,\n",
        "        'capital-loss': capital_loss,\n",
        "        'hours-per-week': hours_per_week,\n",
        "        'native-country': native_country\n",
        "    }])\n",
        "\n",
        "    # Preprocess\n",
        "    input_processed = input_data.copy()\n",
        "    \n",
        "    # One-hot encode (same as training)\n",
        "    input_encoded = pd.get_dummies(input_processed, columns=categorical_cols, drop_first=True)\n",
        "    \n",
        "    # Align columns with training data\n",
        "    for col in feature_names:\n",
        "        if col not in input_encoded.columns:\n",
        "            input_encoded[col] = 0\n",
        "    input_encoded = input_encoded[feature_names]\n",
        "    \n",
        "    # Scale\n",
        "    input_scaled = scaler.transform(input_encoded)\n",
        "    \n",
        "    # Predict\n",
        "    prediction = best_model.predict(input_scaled)[0]\n",
        "    prediction_proba = best_model.predict_proba(input_scaled)[0]\n",
        "    \n",
        "    # Decode prediction\n",
        "    result = le_target.inverse_transform([prediction])[0]\n",
        "    confidence = prediction_proba[prediction] * 100\n",
        "    \n",
        "    return f\"üéØ D·ª± ƒëo√°n: **{result}**\\nüìä ƒê·ªô tin c·∫≠y: **{confidence:.2f}%**\"\n",
        "\n",
        "# Get unique values for dropdowns\n",
        "workclass_options = sorted(X_raw['workclass'].dropna().unique().tolist())\n",
        "education_options = sorted(X_raw['education'].dropna().unique().tolist())\n",
        "marital_options = sorted(X_raw['marital-status'].dropna().unique().tolist())\n",
        "occupation_options = sorted(X_raw['occupation'].dropna().unique().tolist())\n",
        "relationship_options = sorted(X_raw['relationship'].dropna().unique().tolist())\n",
        "race_options = sorted(X_raw['race'].dropna().unique().tolist())\n",
        "sex_options = sorted(X_raw['sex'].dropna().unique().tolist())\n",
        "country_options = sorted(X_raw['native-country'].dropna().unique().tolist())\n",
        "\n",
        "# Create Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=predict_income,\n",
        "    inputs=[\n",
        "        gr.Slider(17, 90, value=39, label=\"Tu·ªïi\"),\n",
        "        gr.Dropdown(workclass_options, value=workclass_options[0], label=\"Workclass\"),\n",
        "        gr.Dropdown(education_options, value=education_options[0], label=\"Education\"),\n",
        "        gr.Dropdown(marital_options, value=marital_options[0], label=\"Marital Status\"),\n",
        "        gr.Dropdown(occupation_options, value=occupation_options[0], label=\"Occupation\"),\n",
        "        gr.Dropdown(relationship_options, value=relationship_options[0], label=\"Relationship\"),\n",
        "        gr.Dropdown(race_options, value=race_options[0], label=\"Race\"),\n",
        "        gr.Dropdown(sex_options, value=sex_options[0], label=\"Sex\"),\n",
        "        gr.Number(value=0, label=\"Capital Gain\"),\n",
        "        gr.Number(value=0, label=\"Capital Loss\"),\n",
        "        gr.Slider(1, 99, value=40, label=\"Hours per Week\"),\n",
        "        gr.Dropdown(country_options, value=\"United-States\", label=\"Native Country\")\n",
        "    ],\n",
        "    outputs=gr.Markdown(label=\"K·∫øt qu·∫£ d·ª± ƒëo√°n\"),\n",
        "    title=\"üéØ Adult Income Prediction\",\n",
        "    description=f\"D·ª± ƒëo√°n m·ª©c thu nh·∫≠p s·ª≠ d·ª•ng **{best_model_name}** (F1-Score: {best_f1:.4f})\",\n",
        "    examples=[\n",
        "        [39, \"State-gov\", \"Bachelors\", \"Never-married\", \"Adm-clerical\",\n",
        "         \"Not-in-family\", \"White\", \"Male\", 2174, 0, 40, \"United-States\"],\n",
        "        [50, \"Self-emp-not-inc\", \"Bachelors\", \"Married-civ-spouse\", \"Exec-managerial\",\n",
        "         \"Husband\", \"White\", \"Male\", 0, 0, 13, \"United-States\"],\n",
        "    ],\n",
        "    theme=gr.themes.Soft()\n",
        ")\n",
        "\n",
        "# Launch\n",
        "demo.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. K·∫øt lu·∫≠n v√† Nh·∫≠n x√©t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìù K·∫æT LU·∫¨N V√Ä NH·∫¨N X√âT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ ·∫¢NH H∆Ø·ªûNG C·ª¶A TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU:\")\n",
        "print(\"-\" * 80)\n",
        "print(\"   ‚úÖ C√°c b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω ƒë√£ c·∫£i thi·ªán ƒë√°ng k·ªÉ hi·ªáu nƒÉng m√¥ h√¨nh:\")\n",
        "print(\"      ‚Ä¢ X·ª≠ l√Ω missing values gi√∫p tr√°nh l·ªói v√† c·∫£i thi·ªán t√≠nh ·ªïn ƒë·ªãnh\")\n",
        "print(\"      ‚Ä¢ X·ª≠ l√Ω outliers gi·∫£m nhi·ªÖu v√† tƒÉng kh·∫£ nƒÉng t·ªïng qu√°t h√≥a\")\n",
        "print(\"      ‚Ä¢ One-Hot Encoding gi√∫p m√¥ h√¨nh hi·ªÉu t·ªët h∆°n categorical features\")\n",
        "print(\"      ‚Ä¢ Scaling chu·∫©n h√≥a gi√∫p thu·∫≠t to√°n h·ªôi t·ª• nhanh h∆°n\")\n",
        "print(\"      ‚Ä¢ SMOTE c√¢n b·∫±ng d·ªØ li·ªáu, c·∫£i thi·ªán Recall cho class thi·ªÉu s·ªë\")\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£ SO S√ÅNH HI·ªÜU NƒÇNG:\")\n",
        "print(\"-\" * 80)\n",
        "for name in models.keys():\n",
        "    acc_no = results_no_prep[name]['accuracy']\n",
        "    acc_with = results_with_prep[name]['accuracy']\n",
        "    f1_no = results_no_prep[name]['f1']\n",
        "    f1_with = results_with_prep[name]['f1']\n",
        "    \n",
        "    print(f\"   ‚Ä¢ {name}:\")\n",
        "    print(f\"     - Accuracy: {acc_no:.4f} ‚Üí {acc_with:.4f} (Œî {(acc_with-acc_no)*100:+.2f}%)\")\n",
        "    print(f\"     - F1-Score: {f1_no:.4f} ‚Üí {f1_with:.4f} (Œî {(f1_with-f1_no)*100:+.2f}%)\")\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£ M√î H√åNH T·ªêT NH·∫§T:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"   üèÜ {best_model_name}\")\n",
        "print(f\"   üìä F1-Score: {best_f1:.4f}\")\n",
        "print(f\"   üí° L√Ω do:\")\n",
        "if 'Random Forest' in best_model_name:\n",
        "    print(\"      ‚Ä¢ X·ª≠ l√Ω t·ªët c·∫£ numerical v√† categorical features\")\n",
        "    print(\"      ‚Ä¢ Robust v·ªõi outliers v√† overfitting\")\n",
        "    print(\"      ‚Ä¢ Feature importance gi√∫p hi·ªÉu d·ªØ li·ªáu t·ªët h∆°n\")\n",
        "elif 'Gradient Boosting' in best_model_name:\n",
        "    print(\"      ‚Ä¢ Boosting gi√∫p c·∫£i thi·ªán t·ª´ng b∆∞·ªõc\")\n",
        "    print(\"      ‚Ä¢ T·ªët v·ªõi d·ªØ li·ªáu ph·ª©c t·∫°p v√† nhi·ªÅu features\")\n",
        "    print(\"      ‚Ä¢ Hi·ªáu nƒÉng cao nh∆∞ng c·∫ßn nhi·ªÅu t√†i nguy√™n\")\n",
        "else:\n",
        "    print(\"      ‚Ä¢ ƒê∆°n gi·∫£n, d·ªÖ gi·∫£i th√≠ch\")\n",
        "    print(\"      ‚Ä¢ Nhanh v√† hi·ªáu qu·∫£ v·ªõi d·ªØ li·ªáu tuy·∫øn t√≠nh\")\n",
        "    print(\"      ‚Ä¢ Ph√π h·ª£p v·ªõi b√†i to√°n binary classification\")\n",
        "\n",
        "print(\"\\n4Ô∏è‚É£ KHUY·∫æN NGH·ªä:\")\n",
        "print(\"-\" * 80)\n",
        "print(\"   üìå Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu l√† B·∫ÆT BU·ªòC ƒë·ªÉ c√≥ m√¥ h√¨nh t·ªët\")\n",
        "print(\"   üìå C√¢n b·∫±ng d·ªØ li·ªáu v·ªõi SMOTE c·∫£i thi·ªán Recall ƒë√°ng k·ªÉ\")\n",
        "print(\"   üìå N√™n th·ª≠ nhi·ªÅu m√¥ h√¨nh v√† ch·ªçn d·ª±a tr√™n F1-Score\")\n",
        "print(\"   üìå S·ª≠ d·ª•ng WandB ƒë·ªÉ theo d√µi v√† so s√°nh experiments\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ HO√ÄN TH√ÄNH LAB 02 - B√ÄI 1\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Finish WandB run\n",
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}